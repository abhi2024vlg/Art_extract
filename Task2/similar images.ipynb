{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11038843,"sourceType":"datasetVersion","datasetId":6875793}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing and Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install byol_pytorch -q q q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:53:39.437007Z","iopub.execute_input":"2025-03-30T14:53:39.437250Z","iopub.status.idle":"2025-03-30T14:53:44.501724Z","shell.execute_reply.started":"2025-03-30T14:53:39.437217Z","shell.execute_reply":"2025-03-30T14:53:44.500680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standard library imports\nimport json\nimport math\nimport os\nimport random\n\n# Third-party library imports\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom scipy.spatial.distance import cdist\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm import tqdm\n\n# Deep learning frameworks\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, Subset, random_split\nimport torchvision\nfrom torchvision import models, transforms as T\n\n# Specialized model imports\nfrom byol_pytorch import BYOL\nfrom transformers import CLIPProcessor, CLIPModel\n\n# Initialize models\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:53:44.502823Z","iopub.execute_input":"2025-03-30T14:53:44.503139Z","iopub.status.idle":"2025-03-30T14:54:11.639627Z","shell.execute_reply.started":"2025-03-30T14:53:44.503109Z","shell.execute_reply":"2025-03-30T14:54:11.638642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloader and Dataset","metadata":{}},{"cell_type":"code","source":"class BaseArtDataset(Dataset):\n    \"\"\"Base dataset class for art images\"\"\"\n    def __init__(self, folder_path):\n        # Get all image files in the directory\n        self.image_files = [f for f in os.listdir(folder_path) \n                          if os.path.isfile(os.path.join(folder_path, f)) and \n                          f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n        \n        self.folder_path = folder_path\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def _load_image(self, idx):\n        \"\"\"Load image from file\"\"\"\n        img_path = os.path.join(self.folder_path, self.image_files[idx])\n        return Image.open(img_path).convert('RGB'), img_path\n\n\nclass ByolTrainDataset(BaseArtDataset):\n    \"\"\"Dataset for BYOL training (single image input)\"\"\"\n    def __init__(self, folder_path):\n        super().__init__(folder_path)\n        # Basic transform for BYOL (minimal since BYOL handles augmentations)\n        self.transform = T.Compose([\n            T.Resize((256,256)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __getitem__(self, idx):\n        image, img_path = self._load_image(idx)        \n        # Apply transforms\n        image = self.transform(image)            \n        return image, img_path\n\n\nclass MetricEvalDataset(BaseArtDataset):\n    \"\"\"Dataset for alignment and uniformity metric evaluation (two augmented views of the same image)\"\"\"\n    def __init__(self, folder_path):\n        super().__init__(folder_path)\n        \n        # Augmentation pipeline for evaluation metrics\n        self.transform = T.Compose([\n            T.RandomResizedCrop(256, scale=(0.5, 1.0)),\n            T.RandomHorizontalFlip(),\n            T.RandomApply([T.ColorJitter(0.4, 0.4, 0.2, 0.1)], p=0.5),\n            T.RandomGrayscale(p=0.1),\n            T.RandomApply([T.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.2),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __getitem__(self, idx):\n        image, img_path = self._load_image(idx)\n        \n        # Create two differently augmented versions of the same image\n        img1 = self.transform(image)\n        img2 = self.transform(image)\n            \n        return img1, img2, img_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:54:22.331308Z","iopub.execute_input":"2025-03-30T14:54:22.331674Z","iopub.status.idle":"2025-03-30T14:54:22.342468Z","shell.execute_reply.started":"2025-03-30T14:54:22.331642Z","shell.execute_reply":"2025-03-30T14:54:22.341650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loaders(dataset_path, batch_size=64, train_size=18000, val_size=1000, test_size=1000, num_workers=4):\n    \"\"\"\n    Create train, validation, and test data loaders with different configurations:\n    - Train loader: BYOL-style single images\n    - Val/Test loaders: Pairs of augmented images for metric evaluation\n    \n    Args:\n        dataset_path (str): Path to dataset directory\n        batch_size (int): Batch size for loaders\n        train_size (int): Number of samples for training\n        val_size (int): Number of samples for validation\n        test_size (int): Number of samples for testing\n        num_workers (int): Number of workers for data loading\n        \n    Returns:\n        dict: Dictionary containing train, val, test loaders and datasets\n    \"\"\"\n    # First, get the total list of files and create indices for splitting\n    base_dataset = BaseArtDataset(dataset_path)\n    dataset_size = len(base_dataset)\n    \n    # Check if we have enough samples\n    required_size = train_size + val_size + test_size\n    \n    if dataset_size < required_size:\n        print(f\"Warning: Dataset only has {dataset_size} samples, which is less than the requested {required_size}\")\n        # Adjust sizes proportionally\n        total = train_size + val_size + test_size\n        train_size = int(dataset_size * (train_size / total))\n        val_size = int(dataset_size * (val_size / total))\n        test_size = dataset_size - train_size - val_size\n    \n    # Create indices for the splits\n    indices = list(range(dataset_size))\n    random.shuffle(indices)\n    \n    train_indices = indices[:train_size]\n    val_indices = indices[train_size:train_size + val_size]\n    test_indices = indices[train_size + val_size:train_size + val_size + test_size]\n    \n    # Create the actual datasets with appropriate transformations\n    train_dataset = ByolTrainDataset(dataset_path)\n    val_dataset = MetricEvalDataset(dataset_path)\n    test_dataset = MetricEvalDataset(dataset_path)\n    \n    # Create subsets based on the indices\n    train_subset = Subset(train_dataset, train_indices)\n    val_subset = Subset(val_dataset, val_indices)\n    test_subset = Subset(test_dataset, test_indices)\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_subset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    # Using a smaller batch size for validation/test due to memory concerns (2 copies of each image)\n    eval_batch_size = batch_size // 2\n    \n    val_loader = DataLoader(\n        val_subset,\n        batch_size=eval_batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_subset,\n        batch_size=eval_batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    return {\n        'train_loader': train_loader,\n        'val_loader': val_loader,\n        'test_loader': test_loader,\n        'train_dataset': train_subset,\n        'val_dataset': val_subset,\n        'test_dataset': test_subset\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:54:24.377326Z","iopub.execute_input":"2025-03-30T14:54:24.377668Z","iopub.status.idle":"2025-03-30T14:54:24.386998Z","shell.execute_reply.started":"2025-03-30T14:54:24.377641Z","shell.execute_reply":"2025-03-30T14:54:24.386221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders with appropriate configurations\ndata = create_data_loaders(\n    dataset_path=\"/kaggle/input/nga-unlablled-dataset/nga_images\",\n    batch_size=32,\n    train_size=18000,\n    val_size=1000,\n    test_size=1000\n)\n    \ntrain_loader = data['train_loader']\nval_loader = data['val_loader']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:54:39.592461Z","iopub.execute_input":"2025-03-30T14:54:39.592739Z","iopub.status.idle":"2025-03-30T14:55:28.748073Z","shell.execute_reply.started":"2025-03-30T14:54:39.592719Z","shell.execute_reply":"2025-03-30T14:55:28.747371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation Metric based on the paper https://arxiv.org/abs/2005.10242 , used for validation and test dataloader","metadata":{}},{"cell_type":"code","source":"# Alignment and uniformity metric functions\ndef align_loss(x, y, alpha=2):\n    \"\"\"Measures the average distance between embeddings of positive pairs\"\"\"\n    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n\n\ndef uniform_loss(x, t=2):\n    \"\"\"Measures how uniformly the embeddings are distributed on the unit hypersphere\"\"\"\n    sq_pdist = torch.pdist(x, p=2).pow(2)\n    return sq_pdist.mul(-t).exp().mean().log()\n\n\ndef evaluate_intrinsic_metrics(model, test_loader, device, alpha=2, t=2, lam=1.0):\n    \"\"\"\n    Evaluate intrinsic metrics (alignment and uniformity) on a test loader\n    \n    Args:\n        model: Feature extractor model\n        test_loader: DataLoader yielding (img1, img2) pairs\n        device: torch device\n        alpha: Power for alignment loss\n        t: Temperature for uniformity loss\n        lam: Weight to balance uniformity loss\n        \n    Returns:\n        Tuple of (alignment, uniformity, total_metric)\n    \"\"\"\n    model.eval()\n    alignment_losses = []\n    uniform_losses = []\n    \n    with torch.no_grad():\n        for img1, img2, _ in test_loader:\n            img1 = img1.to(device)\n            img2 = img2.to(device)\n\n            \n\n            \n            # Get embeddings from the model\n            _,emb1 = model(img1, return_embedding = True)\n            _,emb2 = model(img2, return_embedding = True)\n\n            # Compute alignment loss between positive pairs\n            align = align_loss(emb1, emb2, alpha=alpha)\n            \n            # Compute uniformity on both sets of embeddings\n            unif1 = uniform_loss(emb1, t=t)\n            unif2 = uniform_loss(emb2, t=t)\n            unif = (unif1 + unif2) / 2\n            \n            alignment_losses.append(align.item())\n            uniform_losses.append(unif.item())\n    \n    avg_align = sum(alignment_losses) / len(alignment_losses)\n    avg_unif = sum(uniform_losses) / len(uniform_losses)\n    \n    # The total intrinsic \"loss\" (or metric) is given by:\n    total_metric = avg_align + lam * avg_unif\n    \n    return avg_align, avg_unif, total_metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:45:22.239840Z","iopub.execute_input":"2025-03-30T14:45:22.240079Z","iopub.status.idle":"2025-03-30T14:45:22.247338Z","shell.execute_reply.started":"2025-03-30T14:45:22.240058Z","shell.execute_reply":"2025-03-30T14:45:22.246334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model script","metadata":{}},{"cell_type":"code","source":"resnet = models.resnet50(pretrained=True)\n\nlearner = BYOL(\n    resnet,\n    image_size = 256,\n    hidden_layer = 'avgpool',\n    use_momentum = False       # turn off momentum in the target encoder\n).to(device)\n\noptimizer = torch.optim.Adam(learner.parameters(), lr=3e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:57:55.404678Z","iopub.execute_input":"2025-03-30T14:57:55.404976Z","iopub.status.idle":"2025-03-30T14:57:57.980198Z","shell.execute_reply.started":"2025-03-30T14:57:55.404954Z","shell.execute_reply":"2025-03-30T14:57:57.979555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training script","metadata":{}},{"cell_type":"code","source":"warmup_epochs = 10\nepochs = 100\nlearning_rate = 3e-4\neval_every = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:44:46.303245Z","iopub.execute_input":"2025-03-30T14:44:46.303544Z","iopub.status.idle":"2025-03-30T14:44:46.307317Z","shell.execute_reply.started":"2025-03-30T14:44:46.303517Z","shell.execute_reply":"2025-03-30T14:44:46.306546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Results tracking\nmetrics_history = []\nbest_metric = float('inf')\n    \n# Training loop\nfor epoch in range(epochs):\n    # Adjust learning rate with warmup and cosine decay\n    if epoch < warmup_epochs:\n        lr = learning_rate * (epoch + 1) / warmup_epochs\n    else:\n        lr = learning_rate * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (epochs - warmup_epochs)))\n        \n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n        \n    # Training step\n    learner.train()\n    train_losses = []\n    \n    # Add tqdm progress bar\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n    for images, _ in progress_bar:\n        images = images.to(device)\n            \n        loss = learner(images)\n            \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_losses.append(loss.item())\n        \n        # Update progress bar with current loss\n        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{lr:.6f}\")\n        \n    avg_train_loss = sum(train_losses) / len(train_losses)\n    print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}, LR = {lr:.6f}\")\n    \n    # Evaluate metrics periodically\n    if epoch % eval_every == 0 or epoch == epochs - 1:\n        \n        # Evaluate on validation set\n        align, uniform, total = evaluate_intrinsic_metrics(\n            learner, val_loader, device\n        )\n            \n        print(f\"Validation Metrics - Alignment: {align:.4f}, Uniformity: {uniform:.4f}, Total: {total:.4f}\")\n            \n        metrics_history.append({\n            'epoch': epoch,\n            'train_loss': avg_train_loss,\n            'alignment': align,\n            'uniformity': uniform,\n            'total_metric': total\n        })\n        \n        # Save if best model by total metric\n        if total < best_metric:\n            best_metric = total\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': resnet.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'train_loss': avg_train_loss,\n                'metrics': {\n                    'alignment': align,\n                    'uniformity': uniform,\n                    'total': total\n                }\n            }, 'best_byol_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:32:26.299891Z","iopub.execute_input":"2025-03-30T13:32:26.300178Z","iopub.status.idle":"2025-03-30T13:35:55.876912Z","shell.execute_reply.started":"2025-03-30T13:32:26.300155Z","shell.execute_reply":"2025-03-30T13:35:55.875468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_align, test_uniform, test_total = evaluate_intrinsic_metrics(learner, data['test_loader'], device)    \nprint(f\"Final Test Metrics - Alignment: {test_align:.4f}, Uniformity: {test_uniform:.4f}, Total: {test_total:.4f}\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:08:20.297219Z","iopub.execute_input":"2025-03-16T07:08:20.297570Z","iopub.status.idle":"2025-03-16T07:08:38.112154Z","shell.execute_reply.started":"2025-03-16T07:08:20.297540Z","shell.execute_reply":"2025-03-16T07:08:38.111015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visual evaluation and use of CLIP Score","metadata":{}},{"cell_type":"code","source":"def find_top_similar_pairs(model, test_loader, device, top_k=5):\n    \"\"\"\n    Find the top_k most similar image pairs in the test dataset.\n    \n    Args:\n        model: Your trained BYOL model\n        test_loader: Your test dataloader that yields (img1, img2, path)\n        device: Device to run inference on ('cuda' or 'cpu')\n        top_k: Number of most similar pairs to return\n        \n    Returns:\n        List of dictionaries containing the most similar pairs\n    \"\"\"\n    model.eval()\n    \n    # Collect all embeddings and paths\n    all_embeddings = []\n    all_paths = []\n    \n    print(\"Extracting embeddings from test dataset...\")\n    with torch.no_grad():\n        for img1, img2, rest in tqdm(test_loader):\n            \n            if len(rest) > 0:\n                paths = rest  # Get image paths\n            else:\n                # If paths are not provided, create dummy paths\n                paths = [f\"image_{len(all_paths) + i}\" for i in range(len(img1))]\n            \n            img1 = img1.to(device)\n            \n            # Get embeddings for the first augmentation only\n            # (we don't need both augmentations for finding similar pairs)\n            _, emb1 = model(img1, return_embedding=True)\n            \n            all_embeddings.append(emb1.cpu().numpy())\n            all_paths.extend(paths)\n    \n    # Concatenate all embeddings\n    all_embeddings = np.vstack(all_embeddings)\n    \n    print(f\"Extracted embeddings for {len(all_paths)} images\")\n    \n    # Calculate similarity matrix (cosine similarity)\n    print(\"Calculating similarity matrix...\")\n    similarity_matrix = 1 - cdist(all_embeddings, all_embeddings, 'cosine')\n    \n    # Set diagonal to -inf (to exclude self-comparisons)\n    np.fill_diagonal(similarity_matrix, -np.inf)\n    \n    # Find the top-k most similar pairs\n    most_similar_pairs = []\n    \n    print(f\"Finding top {top_k} most similar pairs...\")\n    for _ in range(top_k):\n        # Find the indices of the maximum similarity\n        i, j = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)\n        similarity = similarity_matrix[i, j]\n        \n        # Add the pair to the result\n        most_similar_pairs.append({\n            'image1_idx': i,\n            'image2_idx': j,\n            'image1_path': all_paths[i],\n            'image2_path': all_paths[j],\n            'learner similarity': similarity\n        })\n        \n        # Set this pair's similarity to -inf to exclude it from future consideration\n        similarity_matrix[i, j] = -np.inf\n        similarity_matrix[j, i] = -np.inf\n    \n    return most_similar_pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:24:37.601264Z","iopub.execute_input":"2025-03-30T13:24:37.601607Z","iopub.status.idle":"2025-03-30T13:24:37.609802Z","shell.execute_reply.started":"2025-03-30T13:24:37.601566Z","shell.execute_reply":"2025-03-30T13:24:37.608914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_similar_pairs(similar_pairs):\n    \"\"\"\n    Display the most similar pairs of images\n    \n    Args:\n        similar_pairs: List of dictionaries with similar pair information\n    \"\"\"\n    for i, pair in enumerate(similar_pairs):\n        print(f\"{i+1}. Similarity: {pair['learner similarity']:.4f}\")\n        print(f\"   Image 1: {pair['image1_path']}\")\n        print(f\"   Image 2: {pair['image2_path']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:24:39.736623Z","iopub.execute_input":"2025-03-30T13:24:39.736995Z","iopub.status.idle":"2025-03-30T13:24:39.742031Z","shell.execute_reply.started":"2025-03-30T13:24:39.736968Z","shell.execute_reply":"2025-03-30T13:24:39.741032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_and_display_similar_pairs(model, test_loader, device='cuda', top_k=5):\n    \"\"\"\n    Find and display the top_k most similar image pairs\n    \n    Args:\n        model: Your trained BYOL model\n        test_loader: Your test dataloader\n        device: Device to run inference on\n        top_k: Number of most similar pairs to return\n        \n    Returns:\n        List of dictionaries containing the most similar pairs\n    \"\"\"\n    similar_pairs = find_top_similar_pairs(model, test_loader, device, top_k)\n    display_similar_pairs(similar_pairs)\n    return similar_pairs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:24:40.100953Z","iopub.execute_input":"2025-03-30T13:24:40.101267Z","iopub.status.idle":"2025-03-30T13:24:40.106201Z","shell.execute_reply.started":"2025-03-30T13:24:40.101240Z","shell.execute_reply":"2025-03-30T13:24:40.105265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find and display top 5 most similar pairs\nsimilar_pairs = find_and_display_similar_pairs(\n    model=learner, \n    test_loader=data['test_loader'], \n    device=device, \n    top_k=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:24:40.644750Z","iopub.execute_input":"2025-03-30T13:24:40.645173Z","iopub.status.idle":"2025-03-30T13:24:57.877295Z","shell.execute_reply.started":"2025-03-30T13:24:40.645142Z","shell.execute_reply":"2025-03-30T13:24:57.876179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"similar_pairs ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T13:24:57.878838Z","iopub.execute_input":"2025-03-30T13:24:57.879137Z","iopub.status.idle":"2025-03-30T13:24:57.885590Z","shell.execute_reply.started":"2025-03-30T13:24:57.879112Z","shell.execute_reply":"2025-03-30T13:24:57.884755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_clip_similarity(data):\n    \"\"\"\n    Calculate the CLIP-based cosine similarity for each image pair in the input data.\n    \n    Args:\n        data (list): List of dictionaries with image pair info\n        \n    Returns:\n        list: The same list with added CLIP similarity scores\n    \"\"\"\n    # Load the CLIP model and processor\n    print(\"Loading CLIP model...\")\n    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n    \n    # Process each pair\n    print(\"Processing image pairs...\")\n    for pair in tqdm(data):\n        image1_path = pair['image1_path']\n        image2_path = pair['image2_path']\n        \n        try:\n            # Load the images\n            image1 = Image.open(image1_path).convert(\"RGB\")\n            image2 = Image.open(image2_path).convert(\"RGB\")\n            \n            # Process the images\n            inputs1 = processor(images=image1, return_tensors=\"pt\")\n            inputs2 = processor(images=image2, return_tensors=\"pt\")\n            \n            # Get image features\n            with torch.no_grad():\n                image_features1 = model.get_image_features(**inputs1)\n                image_features2 = model.get_image_features(**inputs2)\n            \n            # Normalize the features\n            image_features1 = image_features1 / image_features1.norm(dim=1, keepdim=True)\n            image_features2 = image_features2 / image_features2.norm(dim=1, keepdim=True)\n            \n            # Calculate cosine similarity\n            clip_similarity = torch.nn.functional.cosine_similarity(image_features1, image_features2).item()\n            \n            # Add CLIP similarity to the pair data\n            pair['clip_similarity'] = clip_similarity\n            \n        except Exception as e:\n            print(f\"Error processing {image1_path} and {image2_path}: {e}\")\n            pair['clip_similarity'] = None\n    \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:42:50.537965Z","iopub.execute_input":"2025-03-30T12:42:50.538320Z","iopub.status.idle":"2025-03-30T12:42:50.547094Z","shell.execute_reply.started":"2025-03-30T12:42:50.538292Z","shell.execute_reply":"2025-03-30T12:42:50.546303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_data = calculate_clip_similarity(similar_pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:42:52.048321Z","iopub.execute_input":"2025-03-30T12:42:52.048691Z","iopub.status.idle":"2025-03-30T12:42:53.786895Z","shell.execute_reply.started":"2025-03-30T12:42:52.048662Z","shell.execute_reply":"2025-03-30T12:42:53.785934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:42:55.304281Z","iopub.execute_input":"2025-03-30T12:42:55.304772Z","iopub.status.idle":"2025-03-30T12:42:55.313773Z","shell.execute_reply.started":"2025-03-30T12:42:55.304726Z","shell.execute_reply":"2025-03-30T12:42:55.312822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_image_pairs(data, output_dir=None, num_pairs=5):\n    \"\"\"\n    Visualize pairs of images with their similarity scores.\n    \n    Args:\n        data (list): List of dictionaries with image pair info and similarity scores\n        output_dir (str): Directory to save visualizations (if None, just displays them)\n        num_pairs (int): Number of pairs to visualize\n    \"\"\"\n    # Create output directory if specified\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Limit to the requested number of pairs\n    pairs_to_visualize = data[:min(num_pairs, len(data))]\n    \n    for i, pair in enumerate(pairs_to_visualize):\n        # Extract information\n        image1_path = pair['image1_path']\n        image2_path = pair['image2_path']\n        model_similarity = pair['learner similarity']\n        clip_similarity = pair['clip_similarity']\n        \n        # Open images\n        try:\n            img1 = Image.open(image1_path).convert('RGB')\n            img2 = Image.open(image2_path).convert('RGB')\n            \n            # Create figure with properly spaced layout\n            fig = plt.figure(figsize=(12, 8))\n            \n            # Use GridSpec with more space between plots\n            gs = GridSpec(3, 2, height_ratios=[4, 1, 2], hspace=0.1)\n            \n            # Display first image\n            ax1 = fig.add_subplot(gs[0, 0])\n            ax1.imshow(img1)\n            ax1.set_title(f\"Image 1 (idx: {pair['image1_idx']})\")\n            ax1.axis('off')\n            \n            # Display second image\n            ax2 = fig.add_subplot(gs[0, 1])\n            ax2.imshow(img2)\n            ax2.set_title(f\"Image 2 (idx: {pair['image2_idx']})\")\n            ax2.axis('off')\n            \n            # Create a separate subplot for the similarity bars that's clearly separated\n            ax3 = fig.add_subplot(gs[2, :])\n            \n            # Create score bars directly in the subplot (no nested axes)\n            labels = ['Model', 'CLIP']\n            values = [model_similarity, clip_similarity]\n            colors = [plt.cm.viridis(model_similarity), plt.cm.viridis(clip_similarity)]\n            \n            y_pos = np.arange(len(labels))\n            \n            # Plot horizontal bars\n            bars = ax3.barh(y_pos, values, color=colors, height=0.4)\n            \n            # Add labels and formatting\n            ax3.set_yticks(y_pos)\n            ax3.set_yticklabels(labels)\n            ax3.set_xlim(0, 1.0)\n            ax3.set_xticks(np.arange(0, 1.1, 0.1))\n            ax3.set_title('Similarity Scores')\n            \n            # Add value labels to the end of each bar\n            for bar, value in zip(bars, values):\n                ax3.text(value + 0.01, bar.get_y() + bar.get_height()/2, \n                         f'{value:.4f}', va='center')\n            \n            plt.suptitle(f\"Image Pair {i+1}\", fontsize=16)\n            plt.tight_layout()\n            \n            # Save or display\n            if output_dir:\n                plt.savefig(os.path.join(output_dir, f\"pair_{i+1}.png\"), dpi=150)\n                plt.close()\n            else:\n                plt.show()\n                \n        except Exception as e:\n            print(f\"Error visualizing pair {i+1}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:43:13.268267Z","iopub.execute_input":"2025-03-30T12:43:13.268625Z","iopub.status.idle":"2025-03-30T12:43:13.278055Z","shell.execute_reply.started":"2025-03-30T12:43:13.268562Z","shell.execute_reply":"2025-03-30T12:43:13.277227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize image pairs\nvisualize_image_pairs(result_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T12:43:14.731881Z","iopub.execute_input":"2025-03-30T12:43:14.732179Z","iopub.status.idle":"2025-03-30T12:43:16.830565Z","shell.execute_reply.started":"2025-03-30T12:43:14.732157Z","shell.execute_reply":"2025-03-30T12:43:16.829643Z"}},"outputs":[],"execution_count":null}]}