{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"# PyTorch Core Libraries\nimport torch               \nimport torch.nn as nn      \nimport torch.nn.functional as F  \nimport numpy as np\nimport time\nimport torch.optim as optim\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:20.231226Z","iopub.execute_input":"2025-03-16T16:58:20.231531Z","iopub.status.idle":"2025-03-16T16:58:23.364985Z","shell.execute_reply.started":"2025-03-16T16:58:20.231502Z","shell.execute_reply":"2025-03-16T16:58:23.364074Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Model code","metadata":{}},{"cell_type":"markdown","source":"## Different Attention Mechanisms","metadata":{}},{"cell_type":"markdown","source":"### ============================= SE Layer =============================\n","metadata":{}},{"cell_type":"code","source":"# https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.366134Z","iopub.execute_input":"2025-03-16T16:58:23.366450Z","iopub.status.idle":"2025-03-16T16:58:23.371802Z","shell.execute_reply.started":"2025-03-16T16:58:23.366430Z","shell.execute_reply":"2025-03-16T16:58:23.371002Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### ============================= ECA Mechansim =============================","metadata":{}},{"cell_type":"code","source":"class eca_layer(nn.Module):\n    \"\"\"Constructs a ECA module.\n    Args:\n        channel: Number of channels of the input feature map\n        k_size: Adaptive selection of kernel size\n        source: https://github.com/BangguWu/ECANet\n    \"\"\"\n    def __init__(self, channel, k_size=3):\n        super(eca_layer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: input features with shape [b, c, h, w]\n        b, c, h, w = x.size()\n\n        # feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n\n        # Two different branches of ECA module\n        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n\n        # Multi-scale information fusion\n        y = self.sigmoid(y)\n\n        return x * y.expand_as(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.373361Z","iopub.execute_input":"2025-03-16T16:58:23.373584Z","iopub.status.idle":"2025-03-16T16:58:23.391478Z","shell.execute_reply.started":"2025-03-16T16:58:23.373563Z","shell.execute_reply":"2025-03-16T16:58:23.390668Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### ============================= CBAM Module =============================","metadata":{}},{"cell_type":"code","source":"class BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU() if relu else None\n        \n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n        super(ChannelGate, self).__init__()\n        self.gate_channels = gate_channels\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n        )\n        self.pool_types = pool_types\n        \n    def forward(self, x):\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == 'avg':\n                avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp(avg_pool)\n            elif pool_type == 'max':\n                max_pool = F.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp(max_pool)\n            elif pool_type == 'lp':\n                lp_pool = F.lp_pool2d(x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n                channel_att_raw = self.mlp(lp_pool)\n            elif pool_type == 'lse':\n                # LSE pool\n                lse_pool = logsumexp_2d(x)\n                channel_att_raw = self.mlp(lse_pool)\n                \n            if channel_att_sum is None:\n                channel_att_sum = channel_att_raw\n            else:\n                channel_att_sum = channel_att_sum + channel_att_raw\n                \n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        return x * scale\n\ndef logsumexp_2d(tensor):\n    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n    return outputs\n\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1)\n\nclass SpatialGate(nn.Module):\n    def __init__(self):\n        super(SpatialGate, self).__init__()\n        kernel_size = 7\n        self.compress = ChannelPool()\n        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n        \n    def forward(self, x):\n        x_compress = self.compress(x)\n        x_out = self.spatial(x_compress)\n        scale = torch.sigmoid(x_out)  # broadcasting\n        return x * scale\n\nclass CBAM(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n        super(CBAM, self).__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n            \n    def forward(self, x):\n        x_out = self.ChannelGate(x)\n        if not self.no_spatial:\n            x_out = self.SpatialGate(x_out)\n        return x_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.392623Z","iopub.execute_input":"2025-03-16T16:58:23.392930Z","iopub.status.idle":"2025-03-16T16:58:23.411817Z","shell.execute_reply.started":"2025-03-16T16:58:23.392898Z","shell.execute_reply":"2025-03-16T16:58:23.411050Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## ============================= ConvLSTM Cell =============================","metadata":{}},{"cell_type":"code","source":"class ConvLSTMCell_layer(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_size=(3, 3), bias=False):\n        \"\"\"\n        Initialize ConvLSTM cell.\n        Parameters\n        ----------\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n        super(ConvLSTMCell_layer, self).__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n\n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=self.kernel_size,\n                              padding=self.padding,\n                              bias=self.bias)\n\n    def forward(self, input_tensor, cur_state):\n        # cur_state is a tuple\n        h_cur, c_cur = cur_state\n\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n        i = torch.sigmoid(cc_i)  # Input gate\n        f = torch.sigmoid(cc_f)  # Forget gate\n        o = torch.sigmoid(cc_o)  # Output gate\n        g = torch.tanh(cc_g)     # Cell gate\n\n        c_next = f * c_cur + i * g  # Update cell state\n        h_next = o * torch.tanh(c_next)  # Update hidden state\n\n        return h_next, c_next","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.412515Z","iopub.execute_input":"2025-03-16T16:58:23.412727Z","iopub.status.idle":"2025-03-16T16:58:23.428297Z","shell.execute_reply.started":"2025-03-16T16:58:23.412708Z","shell.execute_reply":"2025-03-16T16:58:23.427580Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## ============================= ResNet Helper Functions =============================","metadata":{}},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.429017Z","iopub.execute_input":"2025-03-16T16:58:23.429207Z","iopub.status.idle":"2025-03-16T16:58:23.446942Z","shell.execute_reply.started":"2025-03-16T16:58:23.429190Z","shell.execute_reply":"2025-03-16T16:58:23.446288Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"##  ============================= RLA Bottleneck Block =============================\n","metadata":{}},{"cell_type":"code","source":"class RLA_Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, \n                 rla_channel=32, attention_type=None, attention_param=None, \n                 groups=1, base_width=64, dilation=1, norm_layer=None):\n        super(RLA_Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        \n        # `planes * base_width / 64 * cardinality`\n        width = int(planes * (base_width / 64.)) * groups\n        \n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes + rla_channel, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        \n        self.averagePooling = None\n        if downsample is not None and stride != 1:\n            self.averagePooling = nn.AvgPool2d((2, 2), stride=(2, 2))\n        \n        # Initialize attention module based on specified type\n        self.attention_module = None\n        channels = planes * self.expansion\n        \n        if attention_type == 'SE':\n            reduction = 16 if attention_param is None else attention_param\n            self.attention_module = SELayer(channels, reduction)\n        elif attention_type == 'ECA':\n            k_size = 3 if attention_param is None else attention_param\n            self.attention_module = eca_layer(channels, k_size)\n        elif attention_type == 'CBAM':\n            reduction = 16 if attention_param is None else attention_param\n            self.attention_module = CBAM(channels, reduction)\n\n    def forward(self, x, h, c):\n        identity = x\n        \n        x = torch.cat((x, h), dim=1)\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        \n        # Apply attention module if specified\n        if self.attention_module is not None:\n            out = self.attention_module(out)\n        \n        y = out\n        \n        if self.downsample is not None:\n            identity = self.downsample(identity)\n        if self.averagePooling is not None:\n            h = self.averagePooling(h)\n            c = self.averagePooling(c)\n        \n        out += identity\n        out = self.relu(out)\n\n        return out, y, h, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.447596Z","iopub.execute_input":"2025-03-16T16:58:23.447804Z","iopub.status.idle":"2025-03-16T16:58:23.466110Z","shell.execute_reply.started":"2025-03-16T16:58:23.447766Z","shell.execute_reply":"2025-03-16T16:58:23.465461Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### ============================= RLAlstm_ResNet Network =============================\n","metadata":{}},{"cell_type":"code","source":"class RLAlstm_ResNet(nn.Module):\n    \n    def __init__(self, block, layers,artist_classes=1000,genre_classes=1000,style_classes=1000,\n                 rla_channel=32, attention_type=None, attention_params=None,\n                 zero_init_last_bn=True,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        \"\"\"\n        Initialize RLAlstm_ResNet model with flexible attention mechanisms.\n        \n        Parameters:\n        ----------\n        block : nn.Module\n            The block module to use (e.g., RLA_Bottleneck)\n        layers : list\n            Number of blocks in each layer\n        num_classes : int\n            Number of output classes\n        rla_channel : int\n            Number of filters in RLA\n        attention_type : str or None\n            Type of attention to use: 'SE', 'ECA', 'CBAM', or None\n        attention_params : list or None\n            Parameters for attention modules for each stage\n            For SE: reduction ratios\n            For ECA: kernel sizes\n            For CBAM: reduction ratios\n        \"\"\"\n        super(RLAlstm_ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        \n        # Default attention parameters if None provided\n        if attention_params is None and attention_type is not None:\n            if attention_type == 'SE':\n                attention_params = [16, 16, 16, 16]  # Default reduction ratios\n            elif attention_type == 'ECA':\n                attention_params = [3, 5, 7, 9]  # Default kernel sizes\n            elif attention_type == 'CBAM':\n                attention_params = [16, 16, 16, 16]  # Default reduction ratios\n        \n        self.rla_channel = rla_channel\n        self.flops = False\n        self.attention_type = attention_type\n        \n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        conv_outs = [None] * 4\n        recurrent_convs = [None] * 4\n        stages = [None] * 4\n        stage_bns = [None] * 4\n\n        # Create the four stages of the network with appropriate attention mechanisms\n        # For each stage, we use the corresponding attention parameter if provided\n        stages[0], stage_bns[0], conv_outs[0], recurrent_convs[0] = self._make_layer(\n            block, 64, layers[0], \n            rla_channel=rla_channel, \n            attention_type=attention_type, \n            attention_param=None if attention_params is None else attention_params[0]\n        )\n        \n        stages[1], stage_bns[1], conv_outs[1], recurrent_convs[1] = self._make_layer(\n            block, 128, layers[1], \n            rla_channel=rla_channel, \n            attention_type=attention_type, \n            attention_param=None if attention_params is None else attention_params[1],\n            stride=2, \n            dilate=replace_stride_with_dilation[0]\n        )\n        \n        stages[2], stage_bns[2], conv_outs[2], recurrent_convs[2] = self._make_layer(\n            block, 256, layers[2], \n            rla_channel=rla_channel, \n            attention_type=attention_type, \n            attention_param=None if attention_params is None else attention_params[2],\n            stride=2, \n            dilate=replace_stride_with_dilation[1]\n        )\n        \n        stages[3], stage_bns[3], conv_outs[3], recurrent_convs[3] = self._make_layer(\n            block, 512, layers[3], \n            rla_channel=rla_channel, \n            attention_type=attention_type, \n            attention_param=None if attention_params is None else attention_params[3],\n            stride=2, \n            dilate=replace_stride_with_dilation[2]\n        )\n        \n        self.conv_outs = nn.ModuleList(conv_outs)\n        self.recurrent_convs = nn.ModuleList(recurrent_convs)\n        self.stages = nn.ModuleList(stages)\n        self.stage_bns = nn.ModuleList(stage_bns)\n        \n        self.tanh = nn.Tanh()\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc1 = nn.Linear(512 * block.expansion + rla_channel, artist_classes)\n        self.fc2 = nn.Linear(512 * block.expansion + rla_channel, genre_classes)\n        self.fc3 = nn.Linear(512 * block.expansion + rla_channel, style_classes)\n\n        # Initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch\n        if zero_init_last_bn:\n            for m in self.modules():\n                if isinstance(m, RLA_Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                    \n    def _make_layer(self, block, planes, blocks, \n                    rla_channel, attention_type, attention_param, stride=1, dilate=False):\n        \n        conv_out = conv1x1(planes * block.expansion, rla_channel)\n        recurrent_convlstm = ConvLSTMCell_layer(rla_channel, rla_channel, (3, 3))\n        \n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, \n                            rla_channel=rla_channel, \n                            attention_type=attention_type, \n                            attention_param=attention_param,\n                            groups=self.groups,\n                            base_width=self.base_width, \n                            dilation=previous_dilation, \n                            norm_layer=norm_layer))\n        \n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, \n                                rla_channel=rla_channel, \n                                attention_type=attention_type, \n                                attention_param=attention_param,\n                                groups=self.groups,\n                                base_width=self.base_width, \n                                dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        bns = [norm_layer(rla_channel) for _ in range(blocks)]\n\n        return nn.ModuleList(layers), nn.ModuleList(bns), conv_out, recurrent_convlstm\n    \n    def _forward_impl(self, x):\n        # Initial feature extraction\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        # Initialize hidden states\n        batch, _, height, width = x.size()\n        if self.flops:  # For computing FLOPs and params\n            h = torch.zeros(batch, self.rla_channel, height, width)\n            c = torch.zeros(batch, self.rla_channel, height, width)\n        else:\n            h = torch.zeros(batch, self.rla_channel, height, width, device='cuda')\n            c = torch.zeros(batch, self.rla_channel, height, width, device='cuda')\n\n        # Process through stages with RLA\n        for layers, bns, conv_out, recurrent_convlstm in zip(self.stages, self.stage_bns, self.conv_outs, self.recurrent_convs):    \n            for layer, bn in zip(layers, bns):\n                # Forward through RLA bottleneck block\n                x, y, h, c = layer(x, h, c)\n                \n                # RLA module updates\n                y_out = conv_out(y)\n                y_out = bn(y_out)\n                y_out = self.tanh(y_out)\n                \n                # Update hidden states using ConvLSTM\n                h, c = recurrent_convlstm(y_out, (h, c))\n        \n        # Concatenate final feature maps with hidden state\n        x = torch.cat((x, h), dim=1)\n        \n        # Global average pooling and classification\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        artist = self.fc1(x)\n        genre = self.fc2(x)\n        style = self.fc3(x)\n\n        return artist,genre,style\n\n    def forward(self, x):\n        return self._forward_impl(x)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.468300Z","iopub.execute_input":"2025-03-16T16:58:23.468510Z","iopub.status.idle":"2025-03-16T16:58:23.560163Z","shell.execute_reply.started":"2025-03-16T16:58:23.468492Z","shell.execute_reply":"2025-03-16T16:58:23.559234Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### ============================= Model Loading  =============================\n","metadata":{}},{"cell_type":"code","source":"def rlalstm_resnet50(artist_classes=1000,genre_classes=1000,style_classes=1000, rla_channel=32, attention_type=None, attention_params=None):\n    \"\"\"\n    Constructs a RLAlstm_ResNet-50 model with flexible attention mechanisms.\n    \n    Parameters:\n    -----------\n    num_classes : int\n        Number of output classes\n    rla_channel : int\n        Number of channels in RLA module\n    attention_type : str or None\n        Type of attention to use: 'SE', 'ECA', 'CBAM', or None\n    attention_params : list or None\n        Parameters for attention modules for each stage\n        - For SE: reduction ratios (default: [16, 16, 16, 16])\n        - For ECA: kernel sizes (default: [3, 5, 7, 9])\n        - For CBAM: reduction ratios (default: [16, 16, 16, 16])\n    \n    Returns:\n    --------\n    model : RLAlstm_ResNet\n        The initialized model\n    \"\"\"\n    print(f\"Constructing rlalstm_resnet50 with {attention_type} attention...\")\n    model = RLAlstm_ResNet(\n        RLA_Bottleneck, \n        [3, 4, 6, 3], \n        artist_classes=artist_classes,\n        genre_classes=genre_classes,\n        style_classes=style_classes,\n        rla_channel=rla_channel,\n        attention_type=attention_type,\n        attention_params=attention_params\n    )\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.561451Z","iopub.execute_input":"2025-03-16T16:58:23.561766Z","iopub.status.idle":"2025-03-16T16:58:23.581313Z","shell.execute_reply.started":"2025-03-16T16:58:23.561736Z","shell.execute_reply":"2025-03-16T16:58:23.580522Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Model testing script","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Print model summary\ndef model_summary(model):\n    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Model has {num_params:,} trainable parameters\")\n\n# Create dummy data for testing\ndef create_dummy_data(batch_size=8, img_size=224, num_classes=(23, 10, 16)):\n    # Create random images\n    inputs = torch.randn(batch_size, 3, img_size, img_size)\n    \n    # Create random labels for artist, style, and genre\n    artist_labels = torch.randint(0, num_classes[0], (batch_size,))\n    genre_labels = torch.randint(0, num_classes[1], (batch_size,))\n    style_labels = torch.randint(0, num_classes[2], (batch_size,))\n    \n    return inputs, (artist_labels, genre_labels, style_labels)\n\n# Test forward pass\ndef test_forward_pass(model, batch_size=8, img_size=224):\n    print(\"\\n=== Testing Forward Pass ===\")\n    model.eval()  # Set to evaluation mode\n    \n    # Generate dummy data\n    inputs, _ = create_dummy_data(batch_size, img_size)\n    inputs = inputs.to(device)\n    \n    # Measure inference time\n    start_time = time.time()\n    with torch.no_grad():\n        artist_outputs, genre_outputs, style_outputs = model(inputs)\n    inference_time = time.time() - start_time\n    \n    # Print shape information\n    print(f\"Input shape: {inputs.shape}\")\n    print(f\"Artist output shape: {artist_outputs.shape}\")\n    print(f\"Genre output shape: {genre_outputs.shape}\")\n    print(f\"Style output shape: {style_outputs.shape}\")\n    print(f\"Inference time for batch: {inference_time:.4f} seconds\")\n    print(f\"Inference time per image: {inference_time/batch_size:.4f} seconds\")\n    \n    return artist_outputs, genre_outputs, style_outputs\n\n# Test backward pass (training)\ndef test_backward_pass(model, num_iterations=5, batch_size=8, img_size=224, lr=0.001):\n    print(\"\\n=== Testing Backward Pass (Training) ===\")\n    model.train()  # Set to training mode\n    \n    # Define optimizer and loss functions\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training loop\n    total_train_time = 0\n    losses = []\n    \n    for i in range(num_iterations):\n        # Generate dummy data\n        inputs, (artist_labels, genre_labels, style_labels) = create_dummy_data(batch_size, img_size)\n        inputs = inputs.to(device)\n        artist_labels = artist_labels.to(device)\n        genre_labels = genre_labels.to(device)\n        style_labels = style_labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        start_time = time.time()\n        artist_outputs, genre_outputs, style_outputs = model(inputs)\n        \n        # Calculate loss\n        artist_loss = criterion(artist_outputs, artist_labels)\n        genre_loss = criterion(genre_outputs, genre_labels)\n        style_loss = criterion(style_outputs, style_labels)\n        total_loss = artist_loss + genre_loss + style_loss\n        \n        # Backward pass and optimize\n        total_loss.backward()\n        optimizer.step()\n        \n        iteration_time = time.time() - start_time\n        total_train_time += iteration_time\n        \n        # Print iteration details\n        losses.append(total_loss.item())\n        print(f\"Iteration {i+1}/{num_iterations}, Loss: {total_loss.item():.4f}, Time: {iteration_time:.4f}s\")\n    \n    avg_train_time = total_train_time / num_iterations\n    print(f\"\\nAverage training time per iteration: {avg_train_time:.4f} seconds\")\n    print(f\"Final loss: {losses[-1]:.4f}\")\n    \n    return losses\n\n# Memory usage tracking\ndef print_memory_usage():\n    if torch.cuda.is_available():\n        print(\"\\n=== GPU Memory Usage ===\")\n        print(f\"Allocated: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB\")\n        print(f\"Cached: {torch.cuda.memory_reserved(device) / 1024**3:.2f} GB\")\n        torch.cuda.empty_cache()\n\n# Test model on different input resolutions\ndef test_resolutions(model, resolutions=[224, 384, 512]):\n    print(\"\\n=== Testing Different Input Resolutions ===\")\n    for res in resolutions:\n        print(f\"\\nTesting with resolution {res}x{res}\")\n        try:\n            # Reduce batch size for higher resolutions to avoid OOM\n            batch_size = 8 if res <= 224 else (4 if res <= 384 else 2)\n            test_forward_pass(model, batch_size=batch_size, img_size=res)\n            print_memory_usage()\n        except RuntimeError as e:\n            print(f\"Error with resolution {res}x{res}: {e}\")\n\n# Full test suite\ndef run_tests(model):\n    print(\"\\n==== Running Full Test Suite ====\")\n    \n    model_summary(model)\n    \n    # Basic forward pass test\n    outputs = test_forward_pass(model)\n    print_memory_usage()\n    \n    # Test backward pass (training)\n    losses = test_backward_pass(model)\n    print_memory_usage()\n    \n    # Test different resolutions\n    test_resolutions(model)\n    \n    print(\"\\n==== Tests Completed ====\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.582075Z","iopub.execute_input":"2025-03-16T16:58:23.582328Z","iopub.status.idle":"2025-03-16T16:58:23.611114Z","shell.execute_reply.started":"2025-03-16T16:58:23.582309Z","shell.execute_reply":"2025-03-16T16:58:23.610271Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### ============================= Base Model Testing  =============================","metadata":{}},{"cell_type":"code","source":"model = rlalstm_resnet50(artist_classes=23,genre_classes=10,style_classes=16, rla_channel=32)\nmodel.to(device)\n\nprint(\"Base Model testing started\")\n\n# Run the tests\ntry:\n    run_tests(model)\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\nfinally:\n    # Clean up\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\n\nprint(\"Base Model testing completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:23.611954Z","iopub.execute_input":"2025-03-16T16:58:23.612180Z","iopub.status.idle":"2025-03-16T16:58:29.584736Z","shell.execute_reply.started":"2025-03-16T16:58:23.612161Z","shell.execute_reply":"2025-03-16T16:58:29.583849Z"}},"outputs":[{"name":"stdout","text":"Constructing rlalstm_resnet50 with None attention...\nBase Model testing started\n\n==== Running Full Test Suite ====\nModel has 24,149,649 trainable parameters\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.7825 seconds\nInference time per image: 0.0978 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.10 GB\nCached: 0.28 GB\n\n=== Testing Backward Pass (Training) ===\nIteration 1/5, Loss: 8.4728, Time: 0.5036s\nIteration 2/5, Loss: 9.3464, Time: 0.0437s\nIteration 3/5, Loss: 9.1736, Time: 0.0407s\nIteration 4/5, Loss: 10.6813, Time: 0.0422s\nIteration 5/5, Loss: 10.7163, Time: 0.0353s\n\nAverage training time per iteration: 0.1331 seconds\nFinal loss: 10.7163\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 1.42 GB\n\n=== Testing Different Input Resolutions ===\n\nTesting with resolution 224x224\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0127 seconds\nInference time per image: 0.0016 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 0.49 GB\n\nTesting with resolution 384x384\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([4, 3, 384, 384])\nArtist output shape: torch.Size([4, 23])\nGenre output shape: torch.Size([4, 10])\nStyle output shape: torch.Size([4, 16])\nInference time for batch: 0.0419 seconds\nInference time per image: 0.0105 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 0.60 GB\n\nTesting with resolution 512x512\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([2, 3, 512, 512])\nArtist output shape: torch.Size([2, 23])\nGenre output shape: torch.Size([2, 10])\nStyle output shape: torch.Size([2, 16])\nInference time for batch: 0.0555 seconds\nInference time per image: 0.0278 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 0.59 GB\n\n==== Tests Completed ====\nGPU cache cleared\nBase Model testing completed\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### ============================= Advance Model Testing  =============================","metadata":{}},{"cell_type":"code","source":"# With SE attention\nmodel= rlalstm_resnet50(\n    artist_classes=23,\n    genre_classes=10,\n    style_classes=16,\n    rla_channel=32,\n    attention_type='SE',\n    attention_params=[16, 16, 16, 16]  # Reduction ratios for each stage\n)\nmodel.to(device)\n\nprint(\"SE attention Model testing started\")\n\n# Run the tests\ntry:\n    run_tests(model)\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\nfinally:\n    # Clean up\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\n\nprint(\"SE attention Model testing completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:29.585668Z","iopub.execute_input":"2025-03-16T16:58:29.586036Z","iopub.status.idle":"2025-03-16T16:58:30.958371Z","shell.execute_reply.started":"2025-03-16T16:58:29.586012Z","shell.execute_reply":"2025-03-16T16:58:30.957646Z"}},"outputs":[{"name":"stdout","text":"Constructing rlalstm_resnet50 with SE attention...\nSE attention Model testing started\n\n==== Running Full Test Suite ====\nModel has 26,664,593 trainable parameters\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0258 seconds\nInference time per image: 0.0032 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.12 GB\nCached: 0.49 GB\n\n=== Testing Backward Pass (Training) ===\nIteration 1/5, Loss: 8.3382, Time: 0.1570s\nIteration 2/5, Loss: 8.8769, Time: 0.1051s\nIteration 3/5, Loss: 9.1787, Time: 0.0726s\nIteration 4/5, Loss: 11.4937, Time: 0.0731s\nIteration 5/5, Loss: 11.5075, Time: 0.0535s\n\nAverage training time per iteration: 0.0923 seconds\nFinal loss: 11.5075\n\n=== GPU Memory Usage ===\nAllocated: 0.22 GB\nCached: 1.63 GB\n\n=== Testing Different Input Resolutions ===\n\nTesting with resolution 224x224\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0181 seconds\nInference time per image: 0.0023 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.22 GB\nCached: 0.46 GB\n\nTesting with resolution 384x384\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([4, 3, 384, 384])\nArtist output shape: torch.Size([4, 23])\nGenre output shape: torch.Size([4, 10])\nStyle output shape: torch.Size([4, 16])\nInference time for batch: 0.0198 seconds\nInference time per image: 0.0049 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.22 GB\nCached: 0.52 GB\n\nTesting with resolution 512x512\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([2, 3, 512, 512])\nArtist output shape: torch.Size([2, 23])\nGenre output shape: torch.Size([2, 10])\nStyle output shape: torch.Size([2, 16])\nInference time for batch: 0.0162 seconds\nInference time per image: 0.0081 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.22 GB\nCached: 0.51 GB\n\n==== Tests Completed ====\nGPU cache cleared\nSE attention Model testing completed\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#  With ECA attention\nmodel = rlalstm_resnet50(\n    artist_classes=23,\n    genre_classes=10,\n    style_classes=16,\n    rla_channel=32,\n    attention_type='ECA',\n    attention_params=[3, 5, 7, 9]  # Kernel sizes for each stage\n)\nmodel.to(device)\n\nprint(\"ECA attention Model testing started\")\n\n# Run the tests\ntry:\n    run_tests(model)\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\nfinally:\n    # Clean up\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\n\nprint(\"ECA attention Model testing completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:30.959143Z","iopub.execute_input":"2025-03-16T16:58:30.959438Z","iopub.status.idle":"2025-03-16T16:58:32.170042Z","shell.execute_reply.started":"2025-03-16T16:58:30.959408Z","shell.execute_reply":"2025-03-16T16:58:32.169231Z"}},"outputs":[{"name":"stdout","text":"Constructing rlalstm_resnet50 with ECA attention...\nECA attention Model testing started\n\n==== Running Full Test Suite ====\nModel has 24,149,747 trainable parameters\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0470 seconds\nInference time per image: 0.0059 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.11 GB\nCached: 0.38 GB\n\n=== Testing Backward Pass (Training) ===\nIteration 1/5, Loss: 8.4708, Time: 0.1470s\nIteration 2/5, Loss: 8.6740, Time: 0.0535s\nIteration 3/5, Loss: 9.2979, Time: 0.0440s\nIteration 4/5, Loss: 10.7206, Time: 0.0475s\nIteration 5/5, Loss: 13.0363, Time: 0.0433s\n\nAverage training time per iteration: 0.0671 seconds\nFinal loss: 13.0363\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 1.60 GB\n\n=== Testing Different Input Resolutions ===\n\nTesting with resolution 224x224\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0152 seconds\nInference time per image: 0.0019 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 0.42 GB\n\nTesting with resolution 384x384\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([4, 3, 384, 384])\nArtist output shape: torch.Size([4, 23])\nGenre output shape: torch.Size([4, 10])\nStyle output shape: torch.Size([4, 16])\nInference time for batch: 0.0180 seconds\nInference time per image: 0.0045 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 0.49 GB\n\nTesting with resolution 512x512\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([2, 3, 512, 512])\nArtist output shape: torch.Size([2, 23])\nGenre output shape: torch.Size([2, 10])\nStyle output shape: torch.Size([2, 16])\nInference time for batch: 0.0169 seconds\nInference time per image: 0.0084 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.20 GB\nCached: 0.47 GB\n\n==== Tests Completed ====\nGPU cache cleared\nECA attention Model testing completed\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#  With CBAM attention\nmodel = rlalstm_resnet50(\n    artist_classes=23,\n    genre_classes=10,\n    style_classes=16,\n    rla_channel=32,\n    attention_type='CBAM',\n    attention_params=[16, 16, 16, 16]  # Reduction ratios for each stage\n)\nmodel.to(device)\n\nprint(\"CBAM attention Model testing started\")\n\n# Run the tests\ntry:\n    run_tests(model)\nexcept Exception as e:\n    print(f\"Error during testing: {e}\")\nfinally:\n    # Clean up\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        print(\"GPU cache cleared\")\n\nprint(\"CBAM attention Model testing completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:58:32.170849Z","iopub.execute_input":"2025-03-16T16:58:32.171099Z","iopub.status.idle":"2025-03-16T16:58:33.809939Z","shell.execute_reply.started":"2025-03-16T16:58:32.171078Z","shell.execute_reply":"2025-03-16T16:58:33.809150Z"}},"outputs":[{"name":"stdout","text":"Constructing rlalstm_resnet50 with CBAM attention...\nCBAM attention Model testing started\n\n==== Running Full Test Suite ====\nModel has 26,682,241 trainable parameters\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0508 seconds\nInference time per image: 0.0064 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.12 GB\nCached: 0.44 GB\n\n=== Testing Backward Pass (Training) ===\nIteration 1/5, Loss: 8.5306, Time: 0.1107s\nIteration 2/5, Loss: 8.5557, Time: 0.0749s\nIteration 3/5, Loss: 9.5770, Time: 0.0747s\nIteration 4/5, Loss: 11.7860, Time: 0.0743s\nIteration 5/5, Loss: 10.5449, Time: 0.0747s\n\nAverage training time per iteration: 0.0819 seconds\nFinal loss: 10.5449\n\n=== GPU Memory Usage ===\nAllocated: 0.23 GB\nCached: 1.76 GB\n\n=== Testing Different Input Resolutions ===\n\nTesting with resolution 224x224\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([8, 3, 224, 224])\nArtist output shape: torch.Size([8, 23])\nGenre output shape: torch.Size([8, 10])\nStyle output shape: torch.Size([8, 16])\nInference time for batch: 0.0234 seconds\nInference time per image: 0.0029 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.23 GB\nCached: 0.55 GB\n\nTesting with resolution 384x384\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([4, 3, 384, 384])\nArtist output shape: torch.Size([4, 23])\nGenre output shape: torch.Size([4, 10])\nStyle output shape: torch.Size([4, 16])\nInference time for batch: 0.0254 seconds\nInference time per image: 0.0063 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.23 GB\nCached: 0.72 GB\n\nTesting with resolution 512x512\n\n=== Testing Forward Pass ===\nInput shape: torch.Size([2, 3, 512, 512])\nArtist output shape: torch.Size([2, 23])\nGenre output shape: torch.Size([2, 10])\nStyle output shape: torch.Size([2, 16])\nInference time for batch: 0.0261 seconds\nInference time per image: 0.0130 seconds\n\n=== GPU Memory Usage ===\nAllocated: 0.23 GB\nCached: 0.69 GB\n\n==== Tests Completed ====\nGPU cache cleared\nCBAM attention Model testing completed\n","output_type":"stream"}],"execution_count":14}]}